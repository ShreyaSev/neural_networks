{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416147f4",
   "metadata": {},
   "source": [
    "# Importing the data and splitting it into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3d23c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m43\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(43)\n",
    "#import seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "#seaborn.set(style='whitegrid'); seaborn.set_context('talk')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2911dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a5a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_data['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack((iris_data.data,iris_data.target.T)) #Joins the 4 features and the target variable which is encoded as 0,1,2 for setosa, versicolour and virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80291f99",
   "metadata": {},
   "source": [
    "Since all the categories are one after the other, we need to shuffle it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69853cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d01d0",
   "metadata": {},
   "source": [
    "## train-test split and X and Y split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a06746",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing=train_test_split(data, test_size=0.2, train_size=0.8, random_state=43, shuffle=True, stratify=None)\n",
    "train_X = np.array([i[:4] for i in training])\n",
    "train_y = np.array([i[4] for i in training])\n",
    "test_X = np.array([i[:4] for i in testing])\n",
    "test_y = np.array([i[4] for i in testing])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "train_X = sc.fit_transform(train_X)\n",
    "test_X = sc.transform(test_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018718e1",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82134d",
   "metadata": {},
   "source": [
    "## Activation function - Hyperbolic tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin #base estimator is the base class for all estimators, classifier mixin returns the mean accuracy on the given test data and labels.\n",
    "\n",
    "class MultilayerPerceptron(BaseEstimator,ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.inputLayer = 4                        # number of neurons in Input Layer\n",
    "        self.hiddenLayer = 5                       # number of neurons in Hidden Layer\n",
    "        self.outputLayer = 3                       # number of neurons Output Layer\n",
    "        self.learningRate = 0.001                  # Learning rate\n",
    "        self.max_epochs = 1000                     # Epochs\n",
    "        self.BiasHiddenValue = -1                   # Bias HiddenLayer\n",
    "        self.BiasOutputValue = -1                  # Bias OutputLayer\n",
    "        self.activation = self.tanh # Activation function\n",
    "        self.deriv = self.deriv_tanh\n",
    "        \n",
    "        \n",
    "        #defining the starting weights and biases\n",
    "        self.WEIGHT_hidden = self.starting_weights(self.hiddenLayer, self.inputLayer)\n",
    "        self.WEIGHT_output = self.starting_weights(self.outputLayer, self.hiddenLayer)\n",
    "        self.BIAS_hidden = np.array([self.BiasHiddenValue for i in range(self.hiddenLayer)])\n",
    "        self.BIAS_output = np.array([self.BiasOutputValue for i in range(self.outputLayer)])\n",
    "        self.classes_number = 3 \n",
    "        \n",
    "    \n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def deriv_tanh(self,x):\n",
    "        return (1-(self.tanh(x)**2))\n",
    "    \n",
    "    def starting_weights(self, x, y):\n",
    "        #return [[ random.uniform(0,0.5)for i in range(x)] for j in range(y)]\n",
    "        return [[2  * random.random() - 1 for i in range(x)] for j in range(y)]\n",
    "\n",
    "    \n",
    "    def backpropagation(self,x):\n",
    "        #Step 1- find the error in the output layer, update the weights and bias between output layer and hidden layer\n",
    "        \n",
    "        DELTA_output = []\n",
    "        \n",
    "        ERROR_output = self.output - self.OUTPUT_L2 #expected output- received output\n",
    "        DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2)) # (error*deriv) why -1?\n",
    "        \n",
    "        for i in range(self.hiddenLayer):#loops to connect from each node in hidden layer (i) to each node in output layer (j)\n",
    "            for j in range(self.outputLayer):\n",
    "                self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.OUTPUT_L1[i])) #(error*deriv(j)*output from prev layer(i))\n",
    "                self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])\n",
    "        \n",
    "        #Step 2- find the error in the hidden layer, update the weights and bias between hidden layer and input layer\n",
    "        \n",
    "        delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output)* self.deriv(self.OUTPUT_L1) #formula for delta term for hidden layer= (delta term for output layer multiplied by the weight to the output layer) multiplied by the derivative of the input to the hidden layer \n",
    "        \n",
    "        \n",
    "        for i in range(self.inputLayer):\n",
    "            for j in range(self.hiddenLayer):\n",
    "                self.WEIGHT_hidden[i][j] -= (self.learningRate * (delta_hidden[j] * x[i]))#multiplying the delta term by the input \n",
    "                self.BIAS_hidden[j] -= (self.learningRate * delta_hidden[j])\n",
    "        \n",
    "    def show_err_graphic(self,v_erro,v_epochs):\n",
    "        plt.figure(figsize=(9,4))\n",
    "        plt.plot(v_epochs, v_erro, \"m-\",color=\"b\", marker=11)\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Squared error (MSE) \");\n",
    "        plt.title(\"Error Minimization\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X, y):#Returns the predictions for every element of X\n",
    "        \n",
    "        predictions = []\n",
    "\n",
    "        forward = np.matmul(X,self.WEIGHT_hidden) + self.BIAS_hidden#input to hidden layer\n",
    "        forward = np.matmul(forward, self.WEIGHT_output) + self.BIAS_output#hidden layer to output layer\n",
    "\n",
    "        for i in forward:\n",
    "            predictions.append(max(enumerate(i), key=lambda x:x[1])[0])#WHAT DOES THIS DO\n",
    "\n",
    "        array_score = []# creating output table\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == 0: \n",
    "                array_score.append([i, 'Iris-setosa', predictions[i], y[i]])\n",
    "            elif predictions[i] == 1:\n",
    "                 array_score.append([i, 'Iris-versicolour', predictions[i], y[i]])\n",
    "            elif predictions[i] == 2:\n",
    "                 array_score.append([i, 'Iris-virginica', predictions[i], y[i]])\n",
    "\n",
    "\n",
    "        dataframe = pd.DataFrame(array_score, columns=['_id', 'class', 'output', 'expected_output'])\n",
    "        return predictions, dataframe\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):  \n",
    "        count_epoch = 1\n",
    "        total_error = 0\n",
    "        n = len(X); \n",
    "        epoch_array = []\n",
    "        error_array = []\n",
    "        W0 = []\n",
    "        W1 = []\n",
    "        while(count_epoch <= self.max_epochs):\n",
    "            for idx,inputs in enumerate(X): \n",
    "                #initialize expected input\n",
    "                self.output = np.zeros(self.classes_number)\n",
    "                #forward propogation i.e activation((inputs * weights)+bias)\n",
    "                self.OUTPUT_L1 = self.activation((np.dot(inputs, self.WEIGHT_hidden)) + self.BIAS_hidden.T)\n",
    "                self.OUTPUT_L2 = self.activation(np.dot(self.OUTPUT_L1, self.WEIGHT_output) + self.BIAS_output.T)\n",
    "                #giving values for expected input based on the ground truth we have in y                    \n",
    "                if(y[idx] == 0): \n",
    "                    self.output = np.array([1,0,0]) #Class1 {1,0,0}\n",
    "                elif(y[idx] == 1):\n",
    "                    self.output = np.array([0,1,0]) #Class2 {0,1,0}\n",
    "                elif(y[idx] == 2):\n",
    "                    self.output = np.array([0,0,1]) #Class3 {0,0,1}\n",
    "\n",
    "                square_error = 0\n",
    "                for i in range(self.outputLayer):\n",
    "                    erro=0\n",
    "\n",
    "                    erro = (self.output[i] - self.OUTPUT_L2[i])**2\n",
    "                    square_error = (square_error + (0.05 * erro))#1/2 * error squared\n",
    "                    total_error = total_error + square_error\n",
    "                    \n",
    "                #now, backpropogate\n",
    "                self.backpropagation(inputs)\n",
    "\n",
    "                total_error = (total_error / n)\n",
    "                if((count_epoch % 50 == 0)or(count_epoch == 1)):\n",
    "                    print(\"Epoch \", count_epoch, \"- Total Error: \",total_error)\n",
    "                    error_array.append(total_error)\n",
    "                    epoch_array.append(count_epoch)\n",
    "                    \n",
    "                W0.append(self.WEIGHT_hidden)\n",
    "                W1.append(self.WEIGHT_output)\n",
    "\n",
    "                count_epoch+=1\n",
    "\n",
    "\n",
    "        self.show_err_graphic(error_array,epoch_array)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eaa8ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Perceptron = MultilayerPerceptron()\n",
    "Perceptron.fit(train_X,train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db091aef",
   "metadata": {},
   "source": [
    "# Testing our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbac0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, dataframe = Perceptron.predict(test_X, test_y)\n",
    "hits = n_set = n_vers = n_virg = 0\n",
    "score_set = score_vers = score_virg = 0\n",
    "for j in range(len(test_y)):\n",
    "    if(test_y[j] == 0): \n",
    "        n_set += 1\n",
    "    elif(test_y[j] == 1): \n",
    "        n_vers += 1\n",
    "    elif(test_y[j] == 2): \n",
    "        n_virg += 1\n",
    "        \n",
    "for i in range(len(test_y)):\n",
    "    if test_y[i] == pred[i]: \n",
    "        hits += 1\n",
    "    if test_y[i] == pred[i] and test_y[i] == 0:\n",
    "        score_set += 1\n",
    "    elif test_y[i] == pred[i] and test_y[i] == 1:\n",
    "        score_vers += 1\n",
    "    elif test_y[i] == pred[i] and test_y[i] == 2:\n",
    "        score_virg += 1    \n",
    "         \n",
    "hits = (hits / len(test_y)) * 100\n",
    "faults = 100 - hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea74dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af91523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_hits = []\n",
    "n_samples, n_features = iris_data.data.shape\n",
    "\n",
    "print(\"Percents :\",\"%.2f\"%(hits),\"% hits\",\"and\",\"%.2f\"%(faults),\"% faults\")\n",
    "print(\"Total samples of test\",n_samples)\n",
    "print(\"*Iris-Setosa:\",n_set,\"samples\")\n",
    "print(\"*Iris-Versicolour:\",n_vers,\"samples\")\n",
    "print(\"*Iris-Virginica:\",n_virg,\"samples\")\n",
    "\n",
    "graph_hits.append(hits)\n",
    "graph_hits.append(faults)\n",
    "labels = 'Hits', 'Faults';\n",
    "sizes = [96.5, 3.3]\n",
    "explode = (0, 0.14)\n",
    "\n",
    "fig1, ax1 = plt.subplots();\n",
    "ax1.pie(graph_hits,colors=['green','red'],labels=labels, autopct='%1.1f%%',startangle=90)\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79802e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_set = (score_set/n_set)*100\n",
    "acc_vers = (score_vers/n_vers)*100\n",
    "acc_virg = (score_virg/n_virg)*100\n",
    "print(\"- Accuracy Iris-Setosa:\",\"%.2f\"%acc_set, \"%\")\n",
    "print(\"- Accuracy Iris-Versicolour:\",\"%.2f\"%acc_vers, \"%\")\n",
    "print(\"- Accuracy Iris-Virginica:\",\"%.2f\"%acc_virg, \"%\")\n",
    "names = [\"Setosa\",\"Versicolour\",\"Virginica\"]\n",
    "x1 = [2.0,4.0,6.0]\n",
    "fig, ax = plt.subplots()\n",
    "r1 = plt.bar(x1[0], acc_set,color='orange',label='Iris-Setosa')\n",
    "r2 = plt.bar(x1[1], acc_vers,color='green',label='Iris-Versicolour')\n",
    "r3 = plt.bar(x1[2], acc_virg,color='purple',label='Iris-Virginica')\n",
    "plt.ylabel('Scores %')\n",
    "plt.xticks(x1, names);plt.title('Scores by iris flowers - Multilayer Perceptron')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2125084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa263807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
